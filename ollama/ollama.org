#+STARTUP: content
* ollama
** install

#+begin_src sh
doas pkg install ollama
#+end_src

** run ollama

To run ollama, plese open 2 terminals.

1. In the first terminal, please run:

#+begin_src sh
OLLAMA_NUM_PARALLEL=1 OLLAMA_DEBUG=1 LLAMA_DEBUG=1 ollama start
#+end_src   

2. In the second terminal pull the model you want to install

** models
*** deepseek-r1

[[https://ollama.com/library/deepseek-r1:7b]]

**** pull 

#+begin_src sh
ollama pull deepseek-r1:7b
#+end_src

**** run

#+begin_src sh
ollama run deepseek-r1:7b
#+end_src

*** deepseek-coder

[[https://ollama.com/library/deepseek-coder]]

**** pull 

#+begin_src sh
ollama pull deepseek-coder
#+end_src

**** run

#+begin_src sh
ollama run deepseek-coder
#+end_src

*** deepseek-coder-v2

[[https://ollama.com/library/deepseek-coder-v2/tags]]

**** pull 

#+begin_src sh
ollama pull deepseek-coder-v2:16b-lite-instruct-q4_K_M
#+end_src

**** model file

You can force Ollama to leave more room for your Linux GUI by setting the number of layers sent to the GPU.

***** create the model file

model file name

#+begin_example
deepseek-coder-v2-custom.modelfile
#+end_example

model file code

#+begin_src sh
FROM deepseek-coder-v2:16b-lite-instruct-q4_K_M

# Lowering to 15 layers is a safer bet for a 4GB card
PARAMETER num_gpu 15

# 4096 is enough for a few files of Rust code but small enough for your VRAM.
PARAMETER num_ctx 4096

# Optional: Disable some overhead
PARAMETER f16_kv false
#+end_src

***** create the model from the model file

#+begin_src sh
ollama create deepseek-coder-v2-gpu-15 -f deepseek-coder-v2-custom.modelfile
#+end_src

***** run the custom model

#+begin_src sh
ollama run deepseek-coder-v2-gpu-15
#+end_src

**** run deepseek-coder-v2 without model file

#+begin_src sh
ollama run deepseek-coder-v2:16b-lite-instruct-q4_K_M
#+end_src

*** gemma 3

[[https://ollama.com/library/gemma3]]

**** pull 

#+begin_src sh
ollama pull gemma3:4b
#+end_src

**** run

#+begin_src sh
ollama run gemma3:4b
#+end_src

*** codegemma

[[https://ollama.com/library/codegemma/tags]]

**** pull 

#+begin_src sh
ollama pull codegemma:7b
#+end_src

**** run 

#+begin_src sh
ollama run codegemma:7b
#+end_src

*** llama3.2

[[https://ollama.com/library/llama3.2]]

**** pull 

#+begin_src sh
ollama pull llama3.2
#+end_src

**** run

#+begin_src sh
ollama run llama3.2:latest 
#+end_src

*** mistral

[[https://ollama.com/library/mistral]]

**** pull 

#+begin_src sh
ollama pull mistral
#+end_src

**** run

#+begin_src sh
ollama run mistral:latest
#+end_src

*** qwen2.5

[[https://ollama.com/library/qwen2.5:7b]]

**** pull 

#+begin_src sh
ollama pull qwen2.5:7b
#+end_src

**** run

#+begin_src sh
ollama run qwen2.5:7b
#+end_src

*** zephyr

[[https://ollama.com/library/zephyr]]

**** pull 

#+begin_src sh
ollama pull zephyr
#+end_src

**** run

#+begin_src sh
ollama run zephyr
#+end_src

** kill the ollama server

check if ollama is running

#+begin_src sh
pgrep ollama
#+end_src

kill ollama

#+begin_src sh
pkill ollama
#+end_src
