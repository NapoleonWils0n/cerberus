#+STARTUP: content
* crawl4ai podman
** resources

[[https://github.com/unclecode/crawl4ai]]

[[https://docs.crawl4ai.com/]]

[[https://docs.crawl4ai.com/core/docker-deployment/#option-2-using-docker-compose]]

** open firewall port

make sure to open tcp port 11235 in your firewall

#+begin_example
11235
#+end_example

** directory structure

#+begin_src sh
mkdir -p ~/podman/crawl4ai-podman
#+end_src

** clone crawl4ai git repo

#+begin_src sh
cd ~/podman/crawl4ai-podman
#+end_src

clone the crawl4ai git repo

#+begin_src sh
git clone https://github.com/unclecode/crawl4ai.git
#+end_src

** docker-compose.yml

make sure you are in this directory

#+begin_example
~/podman/crawl4ai-podman
#+end_example

copy the docker-compose.yml from the crawl4ai git repo into the crawl4ai-podman directory

#+begin_src sh
cp crawl4ai/docker-compose.yml .
#+end_src

*** edit docker-compose.yml

we need to change the context path

from

#+begin_src yaml
    context: .
#+end_src

to 

#+begin_src yaml
      context: ./crawl4ai  # <--- MODIFIED TO POINT TO THE SUBDIRECTORY
#+end_src

also create a output_data directory for files

#+begin_src yaml
    - ./output_data:/app/output # output_data directory
#+end_src

and change the image

#+begin_src yaml
    image: unclecode/crawl4ai:latest
#+end_src

as shown below

#+begin_src yaml
version: '3.8'

# Shared configuration for all environments
x-base-config: &base-config
  ports:
    - "11235:11235"  # Gunicorn port
  env_file:
    - .llm.env       # API keys (create from .llm.env.example)
  environment:
    - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
    - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
    - GROQ_API_KEY=${GROQ_API_KEY:-}
    - TOGETHER_API_KEY=${TOGETHER_API_KEY:-}
    - MISTRAL_API_KEY=${MISTRAL_API_KEY:-}
    - GEMINI_API_TOKEN=${GEMINI_API_TOKEN:-}
    - LLM_PROVIDER=${LLM_PROVIDER:-}  # Optional: Override default provider (e.g., "anthropic/claude-3-opus")
  volumes:
    - /dev/shm:/dev/shm  # Chromium performance
    - ./output_data:/app/output # output_data directory
    - ./scripts:/home/appuser/bin # Map local 'scripts' directory to '~/bin' inside the container
  deploy:
    resources:
      limits:
        memory: 4G
      reservations:
        memory: 1G
  restart: unless-stopped
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:11235/health"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 40s
  user: "appuser"

services:
  crawl4ai:
    # 1. Default: Pull multi-platform test image from Docker Hub
    # 2. Override with local image via: IMAGE=local-test docker compose up
    #image: ${IMAGE:-unclecode/crawl4ai:${TAG:-latest}}
    image: unclecode/crawl4ai:latest
    
    # Local build config (used with --build)
    build:
      context: ./crawl4ai  # <--- MODIFIED TO POINT TO THE SUBDIRECTORY
      dockerfile: Dockerfile
      args:
        INSTALL_TYPE: ${INSTALL_TYPE:-default}
        ENABLE_GPU: ${ENABLE_GPU:-false}
    
    # Inherit shared config
    <<: *base-config
#+end_src

** Environment Setup

#+begin_src sh
cp crawl4ai/deploy/docker/.llm.env.example .llm.env
#+end_src

*** edit the .llm.env

#+begin_src sh
# LLM Provider Keys
OPENAI_API_KEY=your_openai_key_here
DEEPSEEK_API_KEY=your_deepseek_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
GROQ_API_KEY=your_groq_key_here
TOGETHER_API_KEY=your_together_key_here
MISTRAL_API_KEY=your_mistral_key_here
GEMINI_API_TOKEN=your_gemini_key_here

# Optional: Override the default LLM provider
# Examples: "openai/gpt-4", "anthropic/claude-3-opus", "deepseek/chat", etc.
# If not set, uses the provider specified in config.yml (default: openai/gpt-4o-mini)
# LLM_PROVIDER=anthropic/claude-3-opus
#+end_src

**** .llm.env google gemini

replace your_gemini_key_here with your gemini api token

using google/gemini-2.5-flash

#+begin_src sh
# LLM Provider Keys
#OPENAI_API_KEY=your_openai_key_here
#DEEPSEEK_API_KEY=your_deepseek_key_here
#ANTHROPIC_API_KEY=your_anthropic_key_here
#GROQ_API_KEY=your_groq_key_here
#TOGETHER_API_KEY=your_together_key_here
#MISTRAL_API_KEY=your_mistral_key_here
GEMINI_API_TOKEN=your_gemini_key_here

# Optional: Override the default LLM provider
# Examples: "openai/gpt-4", "anthropic/claude-3-opus", "deepseek/chat", etc.
# If not set, uses the provider specified in config.yml (default: openai/gpt-4o-mini)
# LLM_PROVIDER=anthropic/claude-3-opus
LLM_PROVIDER=google/gemini-2.5-flash
#+end_src

** setup-crawl4ai

run the setup-crawl4ai script to create the output_data directory

#+begin_src sh
./setup-crawl4ai
#+end_src

setup-crawl4ai

#+begin_src sh
#!/bin/sh

# === Configuration ===
# Define the base directory of your crawl4ai-podman setup
CRAWL4AI_BASE_DIR="$HOME/podman/crawl4ai-podman"
CRAWL4AI_DATA_DIR="$CRAWL4AI_BASE_DIR/output_data"
CRAWL4AI_SCRIPTS_DIR="$CRAWL4AI_BASE_DIR/scripts"

# === Setup Steps ===

# 1. Create the data and scripts directories
echo "Creating data directory: $CRAWL4AI_DATA_DIR"
mkdir -p "$CRAWL4AI_DATA_DIR"
mkdir -p "$CRAWL4AI_SCRIPTS_DIR"

# 2. Set initial permissive permissions (777 is often required for podman/docker on first setup)
echo "Setting initial directory permissions to 777..."
chmod 777 "$CRAWL4AI_DATA_DIR"
chmod 777 "$CRAWL4AI_SCRIPTS_DIR"

# 3. Change ownership using podman unshare
# This is crucial for fixing UID/GID mapping issues. 
# We'll assume the container's 'appuser' belongs to a group that you want to grant access to, 
# typically your primary user group or a specific group like 'users' or 'nogroup'.
echo "Changing ownership (group to 'users') using podman unshare..."
# Replace 'users' with the group that works for your Podman configuration if necessary.
podman unshare chown -R :users "$CRAWL4AI_DATA_DIR"
podman unshare chown -R :users "$CRAWL4AI_SCRIPTS_DIR"

# 4. Set default ACL for future files
# This ensures any new files created by the host or the container inherit group-write permission.
echo "Setting default ACL for new files to be group-editable..."
podman unshare setfacl -d -m g::rwx "$CRAWL4AI_DATA_DIR"
podman unshare chgrp -R 999 "$CRAWL4AI_SCRIPTS_DIR"
echo "Crawl4AI data directory setup complete!"
#+end_src

** scripts
*** screenshot.py

run the setup-crawl4ai which creates the output-data and scripts directories

copy the screenshot.py file to the scripts directory

#+begin_src python
import os
import sys
import asyncio
import argparse
from urllib.parse import urlparse
from base64 import b64decode
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, BrowserConfig

# Define the absolute output directory inside the container
OUTPUT_DIR = "/app/output"

# Adjust paths as needed (Existing boilerplate, left as is)
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)

def get_filename_from_url(url: str) -> str:
    """
    Extracts the last path segment of a URL, cleans it, and uses it as a filename base.
    Removes trailing slashes, any extensions, and query parameters.
    """
    try:
        # Parse the URL to get the path component
        parsed_url = urlparse(url)
        path = parsed_url.path
        
        # Strip trailing slash to correctly identify the filename segment
        path_cleaned = path.rstrip('/')
        
        if not path_cleaned or path_cleaned == '/':
            # Use the domain name if path is root or empty
            # Simple sanitization for domain to ensure it's a valid filename
            filename = parsed_url.netloc.replace('www.', '').replace('.', '_').replace('-', '_')
        else:
            # Use os.path.basename on the cleaned path to get the last path segment
            filename = os.path.basename(path_cleaned)

        # Simple sanitization: remove query parameters or fragments if present
        filename = filename.split('?')[0].split('#')[0]

        # Remove any extension by keeping only the part before the first dot ('.').
        if '.' in filename:
            filename = filename.split('.', 1)[0]

        # Use a sensible default if the result is empty
        if not filename:
            return "index" 

        return filename

    except Exception:
        # Fallback in case of parsing error
        return "default_page_output"


# --- Argument Parsing Setup ---
def parse_args():
    """Parses command-line arguments for the crawling script."""
    parser = argparse.ArgumentParser(
        description="A Crawl4AI script for crawling pages with control over timeout, wait conditions, and rendering delay."
    )
    parser.add_argument(
        "-u", "--url",
        type=str,
        required=True,
        help="The URL to crawl (required)."
    )
    parser.add_argument(
        "--page-timeout",
        type=int,
        default=30000,  # Default is 30000ms (30 seconds)
        help="The maximum time (in milliseconds) to wait for the entire page operation. Default is 30000."
    )
    parser.add_argument(
        "--wait_until",
        type=str,
        default='load', # Set default to 'load' for quick completion on simple pages
        help="The desired Playwright page load state (e.g., 'load', 'networkidle'). Used implicitly to determine when the page is ready before applying the delay."
    )
    parser.add_argument(
        "--delay_after_wait",
        type=int,
        default=2000, # Default to 2000ms (2 seconds)
        help="A delay (in milliseconds) to wait after the page is ready before taking the screenshot. Default is 2000."
    )
    return parser.parse_args()

# --- Crawling Logic (Renamed to main for consistency) ---
async def main(url: str, page_timeout: int, wait_until: str | None, filename_base: str, delay_after_wait: int):
    """
    Initializes Crawl4AI, runs the crawl, and saves the output files.
    """
    print(f"--- Starting Crawl ---")
    print(f"URL: {url}")
    print(f"Page Timeout (ms): {page_timeout}")
    print(f"Wait Condition (Page Load State): {wait_until} (Used implicitly)")
    print(f"Delay After Wait (ms): {delay_after_wait} (Applied via JavaScript wait_for)")
    print(f"Output files will use base name: {filename_base}")
    
    # 1. Define browser configuration
    browser_config = BrowserConfig(
        headless=True,
        verbose=False
    )
    
    # 2. Define the per-run configuration
    # Workaround: Combining page load state and post-load delay using a 'js:' prefixed function in 'wait_for'.
    # NOTE: The 'js:' prefix is CRITICAL to tell Crawl4AI to execute the code instead of looking for it as a selector.
    js_delay_wait = f"""js:async () => {{
        // 1. Wait for the page to reach the desired state (e.g., 'networkidle').
        await new Promise(r => document.readyState === 'complete' ? r() : window.addEventListener('load', r));
        
        // 2. Introduce the user-requested delay for lazy-loading/rendering
        await new Promise(resolve => setTimeout(resolve, {delay_after_wait}));
        
        // Signal completion
        return true;
    }}
    """
    
    # PDF generation has been disabled per user request.
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS, 
        page_timeout=page_timeout,
        
        # Use 'wait_for' to execute the combined load/delay logic (Addresses lazy loading)
        wait_for=js_delay_wait, 
        
        pdf=False, # <-- PDF generation disabled
        screenshot=True, 
        
        # Add lazy-loading/rendering parameters
        wait_for_images=True,
        scan_full_page=True,
    )
    
    try:
        # 3. Execute the crawler
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(
                url=url,
                config=run_config
            )

        # 4. Report and Save results
        if result.success:
            # Ensure the output directory exists before saving
            os.makedirs(OUTPUT_DIR, exist_ok=True)

            screenshot_path = os.path.join(OUTPUT_DIR, f"{filename_base}.png")
            
            # Save screenshot (Requires base64 decoding)
            if result.screenshot:
                with open(screenshot_path, "wb") as f:
                    f.write(b64decode(result.screenshot))
                print(f"Screenshot saved to: {screenshot_path}")
            
            print(f"\n✅ Crawl Successful (Status: {result.status_code})")
            
        else:
            print(f"\n❌ Crawl Failed: {result.error_message}")
            if result.error_message and "timeout" in result.error_message.lower():
                print(f"Tip: The crawl exceeded the maximum time limit of {page_timeout / 1000} seconds. Try increasing the --page-timeout value.")
            
    except Exception as e:
        print(f"\nAn unexpected error occurred during crawling: {e}", file=sys.stderr)

# --- Main Execution ---
if __name__ == "__main__":
    args = parse_args()
    
    # Extract filename from the provided URL
    base_filename = get_filename_from_url(args.url)
    
    try:
        # Run the main asynchronous function with all arguments
        asyncio.run(main(args.url, args.page_timeout, args.wait_until, base_filename, args.delay_after_wait))
    except KeyboardInterrupt:
        print("\nCrawl stopped by user.")
    except Exception as e:
        print(f"An unexpected error occurred: {e}", file=sys.stderr)

#+end_src

*** screenshot.py usage

#+begin_example
python ~/bin/screenshot.py -h
usage: screenshot.py [-h] -u URL [--page-timeout PAGE_TIMEOUT] [--wait_until WAIT_UNTIL] [--delay_after_wait DELAY_AFTER_WAIT]

A Crawl4AI script for crawling pages with control over timeout, wait conditions, and rendering delay.

options:
  -h, --help            show this help message and exit
  -u URL, --url URL     The URL to crawl (required).
  --page-timeout PAGE_TIMEOUT
                        The maximum time (in milliseconds) to wait for the entire page operation. Default is 30000.
  --wait_until WAIT_UNTIL
                        The desired Playwright page load state (e.g., 'load', 'networkidle'). Used implicitly to determine when the page is
                        ready before applying the delay.
  --delay_after_wait DELAY_AFTER_WAIT
                        A delay (in milliseconds) to wait after the page is ready before taking the screenshot. Default is 2000.
#+end_example

*** run screenshot.py 

#+begin_src sh
cd ~/podman/crawl4ai-podman
#+end_src

podman compose up

#+begin_src sh
podman-compose up -d
#+end_src

run podman ps

#+begin_src sh
podman ps
#+end_src

it should output the container name

#+begin_example
37d1504bb4b2  localhost/unclecode/crawl4ai:latest  supervisord -c su...  2 hours ago  Up 2 hours (healthy)  0.0.0.0:11235->11235/tcp, 6379/tcp  crawl4ai-podman_crawl4ai_1
#+end_example

**** run screenshot.py from within the container

enter the container

#+begin_src sh
podman exec -it crawl4ai-podman_crawl4ai_1 bash
#+end_src

run the screenshot.py script

#+begin_src sh
python3 ~/bin/screenshot.py -u 'https://www.channel4.com/' --page-timeout=60000
#+end_src

**** run screenshot.py from outside the container

run the screenshot.py script

#+begin_src sh
podman exec crawl4ai-podman_crawl4ai_1 python3 /home/appuser/bin/screenshot.py -u 'https://www.channel4.com/' --page-timeout=60000
#+end_src

**** files location

files are saved to the output-data directory

** podman compose up

#+begin_src sh
cd ~/podman/crawl4ai-podman
#+end_src

podman compose up

#+begin_src sh
podman-compose up -d
#+end_src

** podman compose down

#+begin_src sh
podman-compose down
#+end_src

** open browser

[[http://127.0.0.1:11235/]]
